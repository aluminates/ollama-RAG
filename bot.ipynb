{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing All Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    Use the following pieces of context to answer the question at the end in one sentence.\n",
    "    If you don't know the answer, don't try to make up an answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set a Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_prompt():\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading LLM using Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "    llm = Ollama(\n",
    "        model=\"llama2\", # tried out mistral-7b, llama2, tinyllama\n",
    "        verbose=True,\n",
    "        temperature=0.2,\n",
    "        callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_qa_chain(llm, prompt, vectorstore):\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        chain_type=\"stuff\", # tried refine, map_reduce, map_rerank \n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a QA Bot by Initializing Chain with LLM, Prompt and Retriever (VDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retrieval_qa_bot():\n",
    "    vectorstore = Chroma(persist_directory=os.getenv('DB_PATH'), embedding_function=HuggingFaceEmbeddings())\n",
    "    try:\n",
    "        llm = load_llm()\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load model: {str(e)}\")\n",
    "    qa_prompt = set_custom_prompt()\n",
    "    try:\n",
    "        qa = retrieval_qa_chain(llm, qa_prompt, vectorstore)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to create retrieval QA chain: {str(e)}\")\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Get Response from QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chain_response(chain, message_content):\n",
    "    cb = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    res = chain.invoke(message_content, callbacks=[cb])\n",
    "    print(f\"response: {res}\")\n",
    "    \n",
    "    # exception handling if the bot doesn't know the answer\n",
    "    if \"I don't know\" in res[\"result\"]:\n",
    "        fallback_message = (\n",
    "            \"I'm sorry I don't know the answer to that.\"\n",
    "            \"Please contact support@mail.com for further assistance regarding your query.\"\n",
    "        )\n",
    "        res[\"result\"] = fallback_message\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Interaction with Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_interaction():\n",
    "    chain = create_retrieval_qa_bot()\n",
    "    while True:\n",
    "        user_query = input(\"Enter your query: \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        response = get_chain_response(chain, user_query)\n",
    "        print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages for Document Chunking & Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "import csv\n",
    "from pptx import Presentation\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Process CSV Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_documents(file_path):\n",
    "    documents = []\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # assuming the first row is the header\n",
    "        for row in reader:\n",
    "            content = ' '.join(row)  # joining all columns to form the content\n",
    "            documents.append(Document(page_content=content, metadata={\"source\": file_path}))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Process PPTX Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pptx_documents(file_path):\n",
    "    documents = []\n",
    "    presentation = Presentation(file_path)\n",
    "    for slide in presentation.slides:\n",
    "        slide_texts = []\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                slide_texts.append(shape.text)\n",
    "        content = \"\\n\".join(slide_texts)\n",
    "        documents.append(Document(page_content=content, metadata={\"source\": file_path}))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Process JSON Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_documents(file_path):\n",
    "    documents = []\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "        content = json.dumps(json_data)  # Convert JSON data to a string\n",
    "        documents.append(Document(page_content=content, metadata={\"source\": file_path}))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Load all Documents from Local Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(data_path):\n",
    "    documents = []\n",
    "\n",
    "    # load PDF files\n",
    "    pdf_loader = PyPDFDirectoryLoader(data_path)\n",
    "    pdf_documents = pdf_loader.load()\n",
    "    documents.extend(pdf_documents)\n",
    "    \n",
    "    # load CSV files\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_documents = load_csv_documents(os.path.join(data_path, file))\n",
    "            documents.extend(csv_documents)\n",
    "\n",
    "    # load PPTX files\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith(\".pptx\"):\n",
    "            ppt_documents = load_pptx_documents(os.path.join(data_path, file))\n",
    "            documents.extend(ppt_documents)\n",
    "\n",
    "    # load JSON files\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith(\".json\"):\n",
    "            json_documents = load_json_documents(os.path.join(data_path, file))\n",
    "            documents.extend(json_documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db():\n",
    "    data_path = os.getenv('DATA_PATH')\n",
    "    db_path = os.getenv('DB_PATH')\n",
    "\n",
    "    if not data_path or not db_path:\n",
    "        raise ValueError(\"DATA_PATH or DB_PATH environment variables not set.\")\n",
    "\n",
    "    documents = load_documents(data_path)\n",
    "    print(f\"Processed {len(documents)} pages.\")\n",
    "\n",
    "    text_splitter = NLTKTextSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(texts)} chunks.\")\n",
    "\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=texts,\n",
    "        embedding=HuggingFaceEmbeddings(),\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    vector_store.persist()\n",
    "    print(f\"Vector database persisted at {db_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1814, which is longer than the specified 1024\n",
      "Created a chunk of size 13251, which is longer than the specified 1024\n",
      "Created a chunk of size 1900, which is longer than the specified 1024\n",
      "Created a chunk of size 9390, which is longer than the specified 1024\n",
      "Created a chunk of size 5503, which is longer than the specified 1024\n",
      "Created a chunk of size 11272, which is longer than the specified 1024\n",
      "Created a chunk of size 19601, which is longer than the specified 1024\n",
      "Created a chunk of size 23993, which is longer than the specified 1024\n",
      "Created a chunk of size 14816, which is longer than the specified 1024\n",
      "Created a chunk of size 9439, which is longer than the specified 1024\n",
      "Created a chunk of size 22207, which is longer than the specified 1024\n",
      "Created a chunk of size 37796, which is longer than the specified 1024\n",
      "Created a chunk of size 1868, which is longer than the specified 1024\n",
      "Created a chunk of size 8214, which is longer than the specified 1024\n",
      "Created a chunk of size 7383, which is longer than the specified 1024\n",
      "Created a chunk of size 1848, which is longer than the specified 1024\n",
      "Created a chunk of size 42774, which is longer than the specified 1024\n",
      "Created a chunk of size 48414, which is longer than the specified 1024\n",
      "Created a chunk of size 72513, which is longer than the specified 1024\n",
      "Created a chunk of size 8432, which is longer than the specified 1024\n",
      "Created a chunk of size 3824, which is longer than the specified 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 152 pages.\n",
      "Split into 303 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ria\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database persisted at D:/Chatbot/ollama-RAG2/vectorstore/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ria\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "create_vector_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ria\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`run` not supported when there is not exactly one output key. Got ['result', 'source_documents'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Chatbot\\ollama-RAG2\\bot.ipynb Cell 31\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Chatbot/ollama-RAG2/bot.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     main_interaction()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Chatbot/ollama-RAG2/bot.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Chatbot/ollama-RAG2/bot.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     main()\n",
      "\u001b[1;32md:\\Chatbot\\ollama-RAG2\\bot.ipynb Cell 31\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Chatbot/ollama-RAG2/bot.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Chatbot/ollama-RAG2/bot.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     main_interaction()\n",
      "\u001b[1;32md:\\Chatbot\\ollama-RAG2\\bot.ipynb Cell 31\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Chatbot/ollama-RAG2/bot.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m user_query\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mexit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mquit\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Chatbot/ollama-RAG2/bot.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Chatbot/ollama-RAG2/bot.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m response \u001b[39m=\u001b[39m get_chain_response(chain, user_query)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Chatbot/ollama-RAG2/bot.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(response[\u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;32md:\\Chatbot\\ollama-RAG2\\bot.ipynb Cell 31\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Chatbot/ollama-RAG2/bot.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_chain_response\u001b[39m(chain, message_content):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Chatbot/ollama-RAG2/bot.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     cb \u001b[39m=\u001b[39m CallbackManager([StreamingStdOutCallbackHandler()])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Chatbot/ollama-RAG2/bot.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     res \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49mrun(message_content, callbacks\u001b[39m=\u001b[39;49m[cb])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Chatbot/ollama-RAG2/bot.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresponse: \u001b[39m\u001b[39m{\u001b[39;00mres\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Chatbot/ollama-RAG2/bot.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# exception handling if the bot doesn't know the answer\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain\\chains\\base.py:595\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Convenience method for executing chain.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \n\u001b[0;32m    560\u001b[0m \u001b[39mThe main difference between this method and `Chain.__call__` is that this\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[39m        # -> \"The temperature in Boise is...\"\u001b[39;00m\n\u001b[0;32m    593\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    594\u001b[0m \u001b[39m# Run at start to make sure this is possible/defined\u001b[39;00m\n\u001b[1;32m--> 595\u001b[0m _output_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_output_key\n\u001b[0;32m    597\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs:\n\u001b[0;32m    598\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain\\chains\\base.py:543\u001b[0m, in \u001b[0;36mChain._run_output_key\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    541\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_output_key\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    542\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 543\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    544\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`run` not supported when there is not exactly \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mone output key. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m         )\n\u001b[0;32m    547\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: `run` not supported when there is not exactly one output key. Got ['result', 'source_documents']."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    main_interaction()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
